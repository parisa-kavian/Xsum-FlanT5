{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/parisa-kavian/Xsum-FlanT5/blob/main/xsum_flanT5_large.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install langchain-community langchain transformers datasets rouge bert-score"
   ],
   "metadata": {
    "id": "uq_0nEWcgcVL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Import Libraries and Load Model"
   ],
   "metadata": {
    "id": "50Wgz7FU9tmM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = 'google/flan-t5-large'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "id": "z8o1UCLi5xXO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Load Dataset"
   ],
   "metadata": {
    "id": "bIMkZ3kL6EiG-markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = load_dataset('xsum', trust_remote_code=True)\n",
    "test_dataset = dataset['test']\n",
    "test_df = test_dataset.to_pandas()\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(\"Dataset loaded successfully. Here are the first 5 rows:\")\n",
    "test_df.head()"
   ],
   "metadata": {
    "id": "bIMkZ3kL6EiG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Generate Summaries"
   ],
   "metadata": {
    "id": "generate-summaries-markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a text generation pipeline\n",
    "summarizer = pipeline('text2text-generation', model=model, tokenizer=tokenizer, device=0) # Use device=0 for GPU\n",
    "\n",
    "# Define a function to generate summary for a single text\n",
    "def generate_summary(text):\n",
    "    # The pipeline returns a list of dictionaries\n",
    "    summary_output = summarizer(text, max_length=150, min_length=30, do_sample=False)\n",
    "    return summary_output[0]['generated_text']\n",
    "\n",
    "# Take a smaller sample for faster processing (e.g., first 25 rows)\n",
    "num_samples = 25\n",
    "sample_df = test_df.head(num_samples).copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# Apply the function to the 'document' column\n",
    "# This will take some time to run\n",
    "print(f\"Generating summaries for the first {num_samples} articles...\")\n",
    "sample_df['model_generated'] = sample_df['document'].apply(generate_summary)\n",
    "\n",
    "print(\"Summaries generated successfully!\")\n",
    "sample_df[['summary', 'model_generated']].head()"
   ],
   "metadata": {
    "id": "XzENlN78hFjp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Evaluation Metrics (Rouge, Bleu, BERTScore)"
   ],
   "metadata": {
    "id": "aQg30Dwz-ejr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from bert_score import score\n",
    "\n",
    "# Prepare the generated and reference summaries\n",
    "generated_summaries = sample_df['model_generated'].tolist()\n",
    "reference_summaries = sample_df['summary'].tolist()\n",
    "\n",
    "# --- ROUGE Score ---\n",
    "rouge = Rouge()\n",
    "rouge_scores = rouge.get_scores(generated_summaries, reference_summaries, avg=True)\n",
    "print(\"--- ROUGE Scores ---\")\n",
    "print(rouge_scores)\n",
    "\n",
    "# --- BLEU Score ---\n",
    "# BLEU score requires tokenized text, so we split strings into lists of words\n",
    "reference_bleu = [[text.split()] for text in reference_summaries]\n",
    "generated_bleu = [text.split() for text in generated_summaries]\n",
    "bleu_score = corpus_bleu(reference_bleu, generated_bleu)\n",
    "print(\"\\n--- BLEU Score ---\")\n",
    "print(f\"BLEU Score: {bleu_score}\")\n",
    "\n",
    "# --- BERTScore ---\n",
    "P, R, F1 = score(generated_summaries, reference_summaries, lang=\"en\", verbose=True)\n",
    "print(\"\\n--- BERTScore ---\")\n",
    "print(f\"BERT Precision: {P.mean().item():.4f}\")\n",
    "print(f\"BERT Recall: {R.mean().item():.4f}\")\n",
    "print(f\"BERT F1 Score: {F1.mean().item():.4f}\")"
   ],
   "metadata": {
    "id": "sXdYqU9YG5RK"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "authorship_tag": "ABX9TyNdGnqi3t0dZ/vgYq5zvpwr",
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
